{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e918a9e2",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7398ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import os\n",
    "from rapidfuzz import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b057e10",
   "metadata": {},
   "source": [
    "D√©finition des listes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "017a764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TECHNICAL_SKILLS = [\n",
    "    # Langages\n",
    "    \"python\",\"r\",\"sql\",\"java\",\"c++\",\"scala\",\"javascript\",\"typescript\",\"bash\",\"shell\",\n",
    "    \"vba\",\"go\",\"matlab\",\n",
    "\n",
    "    # Data engineering\n",
    "    \"spark\",\"hadoop\",\"airflow\",\"etl\",\"elt\",\"pipeline de donn√©es\",\"data pipeline\",\n",
    "    \"dataflow\",\"dbt\",\"kafka\",\"bigquery\",\"redshift\",\n",
    "    \"snowflake\",\"databricks\",\"data warehouse\",\"entrep√¥t de donn√©es\",\n",
    "    \"data lake\",\"lac de donn√©es\",\"mod√©lisation de donn√©es\",\"data modeling\",\n",
    "    \"data integration\",\"data governance\",\"gouvernance des donn√©es\",\n",
    "\n",
    "    # BI & Analytics\n",
    "    \"tableau\",\"power bi\",\"looker\",\"data studio\",\"google data studio\",\n",
    "    \"domo\",\"microstrategy\",\"superset\",\"qlik\",\"qlikview\",\"qlik sense\",\n",
    "    \"excel\",\"google sheets\",\"tableaux de bord\",\"dashboard\",\n",
    "    \"visualisation de donn√©es\",\"data visualization\",\"business intelligence\",\"bi\",\n",
    "\n",
    "    # Cloud & DevOps\n",
    "    \"aws\",\"amazon web services\",\"azure\",\"gcp\",\"google cloud\",\n",
    "    \"linux\",\"unix\",\"git\",\"github\",\"docker\",\"kubernetes\",\"k8s\",\"ci/cd\",\n",
    "\n",
    "    # Data science & ML\n",
    "    \"machine learning\",\"apprentissage automatique\",\"deep learning\",\n",
    "    \"apprentissage profond\",\"statistics\",\"statistiques\",\n",
    "    \"nlp\",\"traitement du langage naturel\",\"clustering\",\"classification\",\n",
    "    \"r√©gression\",\"mod√©lisation pr√©dictive\",\"predictive modeling\",\n",
    "    \"ab testing\",\"test a/b\",\"analyse de cohortes\",\n",
    "\n",
    "    # Librairies ML/DS\n",
    "    \"scikit-learn\",\"sklearn\",\"pandas\",\"numpy\",\"scipy\",\"pytorch\",\n",
    "    \"tensorflow\",\"keras\",\"matplotlib\",\"seaborn\",\"plotly\",\"d3.js\",\n",
    "\n",
    "    # Data quality\n",
    "    \"data cleaning\",\"nettoyage de donn√©es\",\"data validation\"\n",
    "]\n",
    "\n",
    "TOOLS_LIST = [\n",
    "    \"tableau\",\"power bi\",\"looker\",\"excel\",\"google sheets\",\"aws\",\"azure\",\"gcp\",\n",
    "    \"google cloud\",\"bigquery\",\"snowflake\",\"databricks\",\"redshift\",\"mysql\",\n",
    "    \"postgresql\",\"postgres\",\"mssql\",\"oracle\",\"jira\",\"confluence\",\"notion\",\n",
    "    \"asana\",\"lucidchart\",\"visio\",\"git\",\"github\",\"gitlab\",\"docker\",\n",
    "    \"kubernetes\",\"airflow\",\"spark\",\"hadoop\",\"google analytics\",\n",
    "    \"matomo\",\"superset\",\"qlik\",\"qlik sense\"\n",
    "]\n",
    "\n",
    "SOFT_SKILLS = [\n",
    "    \"communication\",\"communication √©crite\",\"communication orale\",\n",
    "    \"teamwork\",\"travail en √©quipe\",\"collaboration\",\"leadership\",\n",
    "    \"gestion de projet\",\"organisation\",\"autonomie\",\"ind√©pendance\",\n",
    "    \"problem solving\",\"r√©solution de probl√®mes\",\"esprit critique\",\n",
    "    \"critical thinking\",\"analyse\",\"analytical thinking\",\n",
    "    \"time management\",\"gestion du temps\",\"adaptabilit√©\",\n",
    "    \"cr√©ativit√©\",\"rigueur\",\"attention aux d√©tails\",\n",
    "    \"initiative\",\"prise de d√©cision\",\"curiosit√©\",\n",
    "    \"pr√©sentation\",\"presentation skills\",\"gestion des parties prenantes\",\n",
    "    \"stakeholder management\"\n",
    "]\n",
    "\n",
    "EDUCATION_LEVELS = {\n",
    "    \"phd\": [\"phd\",\"doctorate\",\"doctorat\",\"th√®se\"],\n",
    "    \"master\": [\"master\",\"msc\",\"bac+5\",\"m2\",\"magist√®re\"],\n",
    "    \"bachelor\": [\"bachelor\",\"licence\",\"bsc\",\"ba\",\"bs\",\"bac+3\"],\n",
    "    \"associate\": [\"associate\",\"dut\",\"bts\",\"bac+2\"],\n",
    "    \"highschool\": [\"high school\",\"lyc√©e\",\"baccalaur√©at\"],\n",
    "    \"any\": [\"dipl√¥me\",\"degree\",\"university degree\",\"college degree\"]\n",
    "}\n",
    "\n",
    "BENEFITS = [\n",
    "    \"bonus\",\"prime\",\"equity\",\"stock options\",\"actions\",\n",
    "    \"health insurance\",\"mutuelle\",\"assurance sant√©\",\"401k\",\n",
    "    \"paid time off\",\"cong√©s pay√©s\",\"jours de repos\",\"sick leave\",\n",
    "    \"mental health\",\"sant√© mentale\",\"remote work\",\"t√©l√©travail\",\n",
    "    \"flexible schedule\",\"horaires flexibles\",\"titres restaurant\",\n",
    "    \"gym\",\"sport\",\"parental leave\",\"cong√© parental\"\n",
    "]\n",
    "\n",
    "SENIORITY_LEVELS = {\n",
    "    \"intern\": [\n",
    "        r\"\\bstagiaire\\b\",\n",
    "        r\"\\ben stage\\b\",\n",
    "        r\"\\bstage \\b\",\n",
    "        r\"\\bintern\\b\",\n",
    "        r\"\\binternship\\b\",\n",
    "        r\"\\balternance\\b\"\n",
    "    ],\n",
    "    \"junior\": [\n",
    "        r\"\\bjunior\\b\",\n",
    "        r\"entry level\",\n",
    "        r\"0-1 ans?\", r\"1-2 ans?\",\n",
    "        r\"d√©butant\"\n",
    "    ],\n",
    "    \"mid\": [\n",
    "        r\"2-4 ans?\", r\"3-5 ans?\",\n",
    "        r\"\\bconfirm√©\\b\",\n",
    "        r\"\\binterm√©diaire\\b\",\n",
    "        r\"\\bmid\\b\"\n",
    "    ],\n",
    "    \"senior\": [\n",
    "        r\"\\bsenior\\b\",\n",
    "        r\"\\biii\\b\",\n",
    "        r\"5\\+? ans?\",\n",
    "        r\"6\\+? ans?\",\n",
    "        r\"exp√©riment√©\"\n",
    "    ],\n",
    "    \"lead\": [\n",
    "        r\"\\blead\\b\",\n",
    "        r\"\\bprincipal\\b\",\n",
    "        r\"\\bhead\\b\",\n",
    "        r\"\\bmanager\\b\",\n",
    "        r\"chef de projet\"\n",
    "    ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05428361",
   "metadata": {},
   "source": [
    "Fonctions de nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ce626e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r'<[^>]*>', ' ', text)\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'[\\r\\n\\t]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text.lower()\n",
    "\n",
    "def drop_invalid_rows(df):\n",
    "    df = df.dropna(subset=[\"description\"])\n",
    "    df = df[df[\"description\"].str.strip() != \"\"]\n",
    "    df = df[df[\"description\"].apply(lambda x: len(x) > 30)]\n",
    "    return df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d8aad",
   "metadata": {},
   "source": [
    "Fonctions d'extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "376b56d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keywords(text, keyword_list):\n",
    "    found = []\n",
    "    for k in keyword_list:\n",
    "        if re.search(r'\\b' + re.escape(k) + r'\\b', text):\n",
    "            found.append(k)\n",
    "    return list(set(found))\n",
    "\n",
    "def match_category(text, category_dict):\n",
    "    for cat, patterns in category_dict.items():\n",
    "        for p in patterns:\n",
    "            if re.search(p, text.lower()):\n",
    "                return cat\n",
    "    return None\n",
    "\n",
    "def parse_experience(text):\n",
    "    match = re.search(r'(\\d+)\\+?\\s*(years?|ans?)', text)\n",
    "    return float(match.group(1)) if match else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e7e3a",
   "metadata": {},
   "source": [
    "correction des salaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1006117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_salary_column(df, col):\n",
    "    corrected = []\n",
    "    for val in df[col]:\n",
    "        try:\n",
    "            if pd.isna(val):\n",
    "                corrected.append(None)\n",
    "            elif val < 1000:  \n",
    "                corrected.append(val * 1000)\n",
    "            else:\n",
    "                corrected.append(val)\n",
    "        except:\n",
    "            corrected.append(None)\n",
    "    df[col] = corrected\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a38531",
   "metadata": {},
   "source": [
    "Pipeline enrichissement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e2ec902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    t = clean_text(text)\n",
    "\n",
    "    features = {\n",
    "        \"technical_skills\": find_keywords(t, TECHNICAL_SKILLS),\n",
    "        \"tools_used\": find_keywords(t, TOOLS_LIST),\n",
    "        \"soft_skills\": find_keywords(t, SOFT_SKILLS),\n",
    "        \"education_level\": match_category(t, EDUCATION_LEVELS),\n",
    "        \"seniority_level\": match_category(t, SENIORITY_LEVELS),\n",
    "        \"benefits\": find_keywords(t, BENEFITS),\n",
    "        \"experience_years\": parse_experience(t)\n",
    "    }\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15078b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMNS = [\n",
    "    \"id\",\"title\",\"company\",\"country\",\"location\",\"link\",\"source\",\n",
    "    \"date_posted\",\"description\",\"description_sans_html\",\n",
    "    \"technical_skills\",\"tools_used\",\"soft_skills\",\n",
    "    \"education_level\",\"seniority_level\",\"benefits\",\n",
    "    \"eeo_statement\",\"hybrid_policy\",\"visa_sponsorship\",\n",
    "    \"tasks\",\"domains\",\"tone_culture\",\"eeo_terms\",\n",
    "    \"experience_mentions\",\"salary_value\",\"salary_type\",\n",
    "    \"salary_currency\",\"experience_years\"\n",
    "]\n",
    "\n",
    "for col in TARGET_COLUMNS:\n",
    "    if col not in df_merged.columns:\n",
    "        df_merged[col] = \"\"\n",
    "\n",
    "df_merged = df_merged[TARGET_COLUMNS]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3736846e",
   "metadata": {},
   "source": [
    "Chargement fichier scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4259a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def clean_and_enrich_scraped(input_csv, output_csv=\"master_enriched.csv\"):\n",
    "    print(\"üì• Chargement du fichier scrapp√©...\")\n",
    "    \n",
    "    # 1. Lecture robuste : on lit tout en 'str' pour √©viter les erreurs de type (ex: 'ID' dans une colonne chiffre)\n",
    "    # on_bad_lines='skip' permet de sauter les lignes corrompues (comme la description cass√©e) sans arr√™ter le script\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            input_csv, \n",
    "            sep=\"\\t\", \n",
    "            dtype=str, \n",
    "            on_bad_lines='skip', \n",
    "            engine='python' # Le moteur python est souvent plus permissif\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur lors de la lecture CSV standard : {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üìä Lignes brutes lues : {len(df)}\")\n",
    "\n",
    "    # 2. Suppression des en-t√™tes r√©p√©t√©s (fusion de plusieurs scrapings)\n",
    "    # On suppose que la premi√®re colonne s'appelle 'id' ou '2275' etc. \n",
    "    # On regarde si la valeur de la colonne est √©gale au nom de la colonne\n",
    "    first_col = df.columns[0]\n",
    "    df = df[df[first_col] != first_col]\n",
    "    \n",
    "    print(\"üßπ Nettoyage des lignes invalides...\")\n",
    "    df = drop_invalid_rows(df)\n",
    "\n",
    "    # 3. Conversion des types num√©riques n√©cessaires\n",
    "    # Convertir les colonnes salaires/id en num√©rique si elles existent, car on a tout lu en 'str'\n",
    "    cols_to_numeric = ['salary_value', 'salary_min', 'salary_max']\n",
    "    for col in cols_to_numeric:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    print(\"‚ú® Enrichissement Regex...\")\n",
    "    enriched_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # On s'assure que la description est bien une chaine de caract√®res\n",
    "        desc = str(row[\"description\"]) if pd.notna(row[\"description\"]) else \"\"\n",
    "        feats = extract_features(desc)\n",
    "        enriched_rows.append({**row.to_dict(), **feats})\n",
    "\n",
    "    df_enriched = pd.DataFrame(enriched_rows)\n",
    "\n",
    "    # Correction des salaires du scraper\n",
    "    if \"salary_value\" in df_enriched.columns:\n",
    "        df_enriched = fix_salary_column(df_enriched, \"salary_value\")\n",
    "\n",
    "    if \"salary_min\" in df_enriched.columns:\n",
    "        df_enriched = fix_salary_column(df_enriched, \"salary_min\")\n",
    "\n",
    "    if \"salary_max\" in df_enriched.columns:\n",
    "        df_enriched = fix_salary_column(df_enriched, \"salary_max\")\n",
    "\n",
    "    print(\"üíæ Sauvegarde du fichier enrichi...\")\n",
    "    # Utilisation de quotechar='\"' et quoting=csv.QUOTE_MINIMAL pour bien g√©rer les textes\n",
    "    df_enriched.to_csv(output_csv, sep=\";\", index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    print(f\"üéâ Dataset enrichi cr√©√© : {output_csv}\")\n",
    "    print(f\"üëâ Lignes finales : {len(df_enriched)}\")\n",
    "\n",
    "    return df_enriched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "137a299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm # Optionnel, pour la barre de chargement\n",
    "\n",
    "def clean_and_enrich_scraped(input_csv, output_csv=\"master_enriched.csv\"):\n",
    "    print(\"üì• Chargement du fichier en mode 'R√©paration'...\")\n",
    "    \n",
    "    # 1. LECTURE ROBUSTE (Modifi√©e pour ignorer les colonnes cass√©es)\n",
    "    # On utilise header=None et quoting=csv.QUOTE_NONE pour lire le fichier brut ligne par ligne\n",
    "    # sans se soucier des d√©calages de colonnes caus√©s par les guillemets ou les fusions.\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "    input_csv,\n",
    "    sep=\";\",\n",
    "    quotechar='\"',\n",
    "    escapechar=\"\\\\\",\n",
    "    dtype=str,\n",
    "    on_bad_lines=\"skip\",\n",
    "    engine=\"python\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur critique : {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üìä Lignes brutes lues : {len(df)}\")\n",
    "    \n",
    "    enriched_rows = []\n",
    "    \n",
    "    print(\"‚ú® Reconstruction et Enrichissement...\")\n",
    "    \n",
    "    # On it√®re sur chaque ligne brute pour reconstruire les colonnes correctement\n",
    "    # (Cela remplace ton √©tape 'drop_invalid_rows' de mani√®re plus intelligente)\n",
    "    \n",
    "    # Barre de progression si tqdm est install√©, sinon it√©rateur simple\n",
    "    iterator = tqdm(df.iterrows(), total=len(df)) if 'tqdm' in globals() else df.iterrows()\n",
    "\n",
    "    for idx, row in iterator:\n",
    "        # On nettoie la ligne : on enl√®ve les 'nan' et les cases vides\n",
    "        values = [str(x) for x in row.values if pd.notna(x) and str(x).lower() != 'nan' and str(x).strip() != \"\"]\n",
    "        \n",
    "        if not values:\n",
    "            continue\n",
    "\n",
    "        # --- LOGIQUE DE R√âCUP√âRATION ---\n",
    "        \n",
    "        # 1. Trouver la Description (C'est toujours le texte le plus long)\n",
    "        try:\n",
    "            description = max(values, key=len)\n",
    "        except ValueError:\n",
    "            continue \n",
    "\n",
    "        # Filtre √©quivalent √† ta fonction 'drop_invalid_rows' : \n",
    "        # Si description < 50 chars, c'est probablement un header ou une erreur -> on saute\n",
    "        if len(description) < 50:\n",
    "            continue\n",
    "            \n",
    "        # 2. R√©cup√©rer les infos fixes (ID, Titre, Entreprise) par position\n",
    "        # M√™me si les colonnes d√©calent √† la fin, le d√©but (0, 1, 2) reste souvent stable\n",
    "        job_id = values[0]\n",
    "        \n",
    "        # Suppression des en-t√™tes r√©p√©t√©s (ex: si l'ID est \"job_id\" ou \"2275\")\n",
    "        if str(job_id).lower() in ['id', 'job_id', 'job id', 'trackingid']:\n",
    "            continue\n",
    "            \n",
    "        title = values[1] if len(values) > 1 else \"Inconnu\"\n",
    "        company = values[2] if len(values) > 2 else \"Inconnu\"\n",
    "        \n",
    "        # 3. R√©cup√©rer Date et Lien (qui bougent souvent) via Regex\n",
    "        # Cherche une date format YYYY-MM-DD\n",
    "        date = next((s for s in values if re.search(r'\\d{4}-\\d{2}-\\d{2}', s)), None)\n",
    "        # Cherche un lien http\n",
    "        link = next((s for s in values if s.startswith('http')), None)\n",
    "\n",
    "        # --- ENRICHISSEMENT (Ton code original) ---\n",
    "        # On appelle ta fonction d'extraction sur la description trouv√©e\n",
    "        feats = extract_features(description)\n",
    "        \n",
    "        # On construit la nouvelle ligne propre\n",
    "        row_data = {\n",
    "            \"id\": job_id,\n",
    "            \"title\": title,\n",
    "            \"company\": company,\n",
    "            \"date\": date,\n",
    "            \"link\": link,\n",
    "            \"description\": description,\n",
    "            # On inclut tous les champs g√©n√©r√©s par extract_features (skills, tools, etc.)\n",
    "            **feats \n",
    "        }\n",
    "        \n",
    "        # Note: Les colonnes salaires d'origine sont souvent perdues dans le d√©calage.\n",
    "        # On ajoute des placeholders pour que 'fix_salary_column' ne plante pas plus tard\n",
    "        row_data[\"salary_value\"] = None\n",
    "        row_data[\"salary_min\"] = None\n",
    "        row_data[\"salary_max\"] = None\n",
    "\n",
    "        enriched_rows.append(row_data)\n",
    "\n",
    "    # Cr√©ation du DataFrame propre\n",
    "    df_enriched = pd.DataFrame(enriched_rows)\n",
    "\n",
    "    # --- NETTOYAGE FINAL (Ton code original) ---\n",
    "    \n",
    "    # 3. Conversion et correction des salaires\n",
    "    # (M√™me si les valeurs sont souvent None ici vu le parsing, on garde la logique au cas o√π)\n",
    "    if \"salary_value\" in df_enriched.columns:\n",
    "        df_enriched = fix_salary_column(df_enriched, \"salary_value\")\n",
    "    if \"salary_min\" in df_enriched.columns:\n",
    "        df_enriched = fix_salary_column(df_enriched, \"salary_min\")\n",
    "    if \"salary_max\" in df_enriched.columns:\n",
    "        df_enriched = fix_salary_column(df_enriched, \"salary_max\")\n",
    "\n",
    "    print(\"üíæ Sauvegarde du fichier enrichi...\")\n",
    "    # Utilisation de QUOTE_ALL pour blinder le fichier de sortie contre les futurs probl√®mes\n",
    "    df_enriched.to_csv(output_csv, sep=\";\", index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "    print(f\"üéâ Dataset enrichi cr√©√© : {output_csv}\")\n",
    "    print(f\"üëâ Lignes finales : {len(df_enriched)}\")\n",
    "\n",
    "    return df_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9876fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Chargement du fichier en mode 'R√©paration'...\n",
      "üìä Lignes brutes lues : 14670\n",
      "‚ú® Reconstruction et Enrichissement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14670/14670 [02:02<00:00, 119.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Sauvegarde du fichier enrichi...\n",
      "üéâ Dataset enrichi cr√©√© : master_enriched.csv\n",
      "üëâ Lignes finales : 14670\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    clean_and_enrich_scraped(\"data.csv\", \"master_enriched.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae7de5",
   "metadata": {},
   "source": [
    "Fusionner avec le premier dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86b15703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV propre g√©n√©r√© ! üéâ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(t):\n",
    "    if pd.isna(t):\n",
    "        return \"\"\n",
    "    t = str(t)\n",
    "    t = t.replace(\"\\r\", \"\")\n",
    "    t = t.replace(\"\\n\", \"\\\\n\")   # OK\n",
    "    t = t.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    return t.strip()\n",
    "\n",
    "\n",
    "def clean_list_column(t):\n",
    "    if pd.isna(t):\n",
    "        return \"\"\n",
    "    t = str(t)\n",
    "    t = t.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    t = t.replace(\"'\", \"\")\n",
    "    parts = re.split(r\"[;,]+\", t)\n",
    "    parts = [p.strip().lower() for p in parts if p.strip()]\n",
    "    return \";\".join(parts)\n",
    "\n",
    "# Lire tes fichiers bruts (avec ; comme s√©parateur)\n",
    "df_site = pd.read_csv(\"november_17_jobs_updated.csv\", sep=\";\", low_memory=False)\n",
    "df_new  = pd.read_csv(\"data.csv\", sep=\";\", low_memory=False)\n",
    "\n",
    "# Harmoniser la colonne 'link'\n",
    "for df in (df_site, df_new):\n",
    "    if \"url\" in df.columns:\n",
    "        df[\"link\"] = df[\"url\"]\n",
    "\n",
    "df_site.drop_duplicates(subset=\"link\", inplace=True)\n",
    "df_new.drop_duplicates(subset=\"link\", inplace=True)\n",
    "\n",
    "df_to_add = df_new[~df_new[\"link\"].isin(df_site[\"link\"])]\n",
    "df_merged = pd.concat([df_site, df_to_add], ignore_index=True)\n",
    "df_merged.drop_duplicates(subset=\"link\", inplace=True)\n",
    "\n",
    "# Nettoyer descriptions\n",
    "df_merged[\"description\"] = df_merged[\"description\"].apply(clean_text)\n",
    "df_merged[\"description_sans_html\"] = df_merged[\"description_sans_html\"].apply(clean_text)\n",
    "\n",
    "# Nettoyer skills\n",
    "df_merged[\"technical_skills\"] = df_merged[\"technical_skills\"].apply(clean_list_column)\n",
    "df_merged[\"tools_used\"]      = df_merged[\"tools_used\"].apply(clean_list_column)\n",
    "df_merged[\"soft_skills\"]     = df_merged[\"soft_skills\"].apply(clean_list_column)\n",
    "\n",
    "# Exporter un CSV propre (s√©parateur s√ªr)\n",
    "\n",
    "\n",
    "df_merged.to_csv(\n",
    "    \"data_universe/data/job_data.csv\",\n",
    "    sep=\";\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8\",\n",
    "    quoting=csv.QUOTE_ALL,\n",
    "    lineterminator=\"\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"CSV propre g√©n√©r√© ! üéâ\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
